{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c3d52-681a-4702-946a-58424d90ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb2f37",
   "metadata": {},
   "source": [
    "### load variable mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"variable_mapping.json\").open(\"r\") as f:\n",
    "    mapping = json.load(f, parse_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the encoded values in the mapping to integers since they get read in as strings\n",
    "for c in mapping.keys():\n",
    "    mapping[c] = {int(k): v for k, v in mapping[c].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dbb9d",
   "metadata": {},
   "source": [
    "### load primary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fddc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"corrected_narrative_primary.csv\",\n",
    "    # set columns that can be null to nullable ints\n",
    "    dtype={\"body_part_2\": \"Int64\", \"diagnosis_2\": \"Int64\"},\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e80fbe",
   "metadata": {},
   "source": [
    "### replace numeric values with corresponding strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df = df.copy()\n",
    "\n",
    "for col in mapping.keys():\n",
    "    decoded_df[col] = decoded_df[col].map(mapping[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79536b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure mappings were applied correctly by checking that the number of missing values did not change\n",
    "assert (decoded_df.isnull().sum() == df.isnull().sum()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ad37b-9614-437b-aac7-64aa5e39e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = decoded_df.sample(n=5000, replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683c71b-ad2f-45b7-899d-3117140c3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_sub = random_sample.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447cd5e-6798-43ff-9250-cd90a6d17860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_sample_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b54147-4454-4311-bb57-1850a3f1871e",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c0b39-9bfb-4453-a9f7-6742d023f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18501e0d-7920-492b-a259-fd0bc34bad42",
   "metadata": {},
   "source": [
    "# My Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91e26a-632b-4f0a-bc62-a4ab88f338b5",
   "metadata": {},
   "source": [
    "Corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628b143-9f57-4b01-9e39-0846c0f2dd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fall = pd.read_csv('fall_set5_corrected_narrative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be50d35-1c6b-47c3-b486-478fd58a0c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82645fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e636cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall.drop_duplicates(inplace=True)\n",
    "len(fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e454cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_nas = fall[fall['span'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf419ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fall_nas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = fall.label.unique()\n",
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_use = ['CHR', 'TRS', 'LAD', 'OBJ', 'RCH', 'SF', 'SHW', 'SO', 'STR', 'SU', 'WT']\n",
    "# labels_to_use = ['SO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_sub = fall[fall['label'].isin(labels_to_use)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_all = pd.concat((fall_sub, fall_nas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5159e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b78415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(99)\n",
    "randomlist = random.sample(range(0, len(fall_all)), len(fall_all), )\n",
    "print(randomlist[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_all_random = fall_all.iloc[randomlist,]\n",
    "fall_all_random.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4489540-39a1-4ac2-90a9-b823642f9c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_list = fall_all_random.key_entry.unique()\n",
    "len(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fall_all_random[fall_all_random.key_entry == key_list[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fe623-37b0-4abb-af94-9453d88ff489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "# special_case = [{Token.ORTH: \"CLOSED-HEAD\"}]\n",
    "# nlp.tokenizer.add_special_case(\"CLOSED-HEAD\", special_case)\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# keeping span token lengths to appropriately set config\n",
    "token_lengths = []\n",
    "label_list = []\n",
    "\n",
    "docs=[] # this will hold the processed strings and spans\n",
    "for k in key_list:\n",
    "    doc = nlp(k)\n",
    "    temp_df = fall_all_random[fall_all_random.key_entry == k]\n",
    "\n",
    "    if len(temp_df)==1:    \n",
    "        if pd.isna(temp_df.iloc[0,2]):\n",
    "            doc.spans[\"sc\"] = []\n",
    "            docs.append(doc)\n",
    "        else:     \n",
    "            span_text = temp_df.iloc[0,1]\n",
    "            temp_label = temp_df.iloc[0,2]       \n",
    "            span_start_char = k.find(span_text)\n",
    "            span_end_char = span_start_char + len(span_text)\n",
    "    \n",
    "            # Finding the start and end tokens using character offsets\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx == span_start_char:\n",
    "                     start_token = token.i\n",
    "                if token.idx + len(token.text) == span_end_char:\n",
    "                    end_token = token.i\n",
    "                    break\n",
    "            if start_token is not None and end_token is not None:\n",
    "                temp_start = start_token\n",
    "                temp_end = end_token + 1\n",
    "                doc.spans[\"sc\"] = [Span(doc, temp_start, temp_end, temp_label)]\n",
    "                docs.append(doc)\n",
    "                token_lengths.append(temp_end - temp_start)\n",
    "                label_list.append(temp_label)\n",
    "            else:\n",
    "                print(k, \"span=\", span_text,\"couldn't find tokens\")\n",
    "    else:\n",
    "        print(\"temp_df has length > 1\")\n",
    "        span_list = []\n",
    "        for ent in range(len(temp_df)):\n",
    "            span_text = temp_df.iloc[ent,1]\n",
    "            temp_label = temp_df.iloc[ent,2]\n",
    "            span_start_char = k.find(span_text)\n",
    "            span_end_char = span_start_char + len(span_text)\n",
    "            print(span_start_char, span_end_char)\n",
    "\n",
    "            # Finding the start and end tokens using character offsets\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx == span_start_char:\n",
    "                     start_token = token.i\n",
    "                if token.idx + len(token.text) == span_end_char:\n",
    "                    end_token = token.i\n",
    "                    break\n",
    "            if start_token is not None and end_token is not None:\n",
    "                temp_start = start_token\n",
    "                temp_end = end_token + 1             \n",
    "                span_list.append(Span(doc, temp_start, temp_end, temp_label))\n",
    "                token_lengths.append(temp_end - temp_start)\n",
    "                label_list.append(temp_label)\n",
    "            else:\n",
    "                print(k, \"span=\",span_text, \"couldn't find tokens\")\n",
    "        \n",
    "        doc.spans[\"sc\"] = span_list\n",
    "    docs.append(doc)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d16e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(token_lengths), np.max(token_lengths), np.median(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a50f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(token_lengths, q =[0.05,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d92095",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(np.array(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888d041",
   "metadata": {},
   "source": [
    "Make training and test sets with the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dec8e9-3243-4929-af99-da59854c1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8299d7f-7b9b-490a-a09c-fd7eb7f0b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin(docs=docs[0:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646813a-597c-4887-b100-53b7354bc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin.to_disk(\"./train_230929_mult_labs1017.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa81e1-08f2-41ae-ae6f-d0d2787322b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin(docs=docs[800:])\n",
    "doc_bin.to_disk(\"./dev_230929_mult_labs1017.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2b248-4078-413a-9622-58674b528e7c",
   "metadata": {},
   "source": [
    "python -m spacy init config ./config.cfg --lang en --pipeline spancat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e220f29-7abc-44bf-8a99-e4a017f84e01",
   "metadata": {},
   "source": [
    "## Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f2df3-c0da-4f9a-ae84-c4dbea3bc93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp_spancat = spacy.load(\"/Users/wendyphillips/Documents/Computing/WendysPython/Falling_analysis/code/outputs230929_mult_labs1017/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56977a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spancat.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cc55b-d2ee-474e-8097-48c2aca45b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_text = random_sample_sub.iloc[2101,1]\n",
    "doc = nlp_spancat(test_text)\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf950fb-98d6-4410-86ba-08dd02939886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for span in doc.spans[\"sc\"]:\n",
    "    print(span.label_, span.start, span.end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa9794-24b6-46b4-bce4-dcc976b099b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_sample_sub2 = random_sample_sub.iloc[2000:2050,]\n",
    "for text in random_sample_sub2['narrative']:\n",
    "    print(text)\n",
    "    doc = nlp_spancat(text)\n",
    "    for span in doc.spans[\"sc\"]:\n",
    "        print(span.label_, span.start, span.end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_sub2 = random_sample_sub.iloc[2000:2500,]\n",
    "\n",
    "# Create an empty DataFrame with column names\n",
    "output_df = pd.DataFrame(columns=['text', 'span_label', 'span_text'])\n",
    "\n",
    "for text in random_sample_sub2['narrative']:\n",
    "    doc = nlp_spancat(text)\n",
    "    \n",
    "    if len(doc.spans[\"sc\"]) == 0:\n",
    "        df2 = pd.DataFrame([[text, \"NA\", \"NA\"]], columns=['text', 'span_text', 'span_label'])\n",
    "        # Append the new row to the DataFrame\n",
    "        output_df = pd.concat([output_df, df2])\n",
    "    else:\n",
    "        for span in doc.spans[\"sc\"]:\n",
    "        # Create a new row as a dictionary\n",
    "        \n",
    "            df2 = pd.DataFrame([[text, span.label_, span.text]], columns=['text', 'span_text', 'span_label'])\n",
    "            # Append the new row to the DataFrame\n",
    "            output_df = pd.concat([output_df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"predictions_1017.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
