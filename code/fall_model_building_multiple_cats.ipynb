{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dc5999",
   "metadata": {},
   "source": [
    "# Investigating clinical notes regarding falls in older adults\n",
    "Author: Wendy Phillips\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "Falls in older adults are frequent and can have major health impacts.\n",
    "\n",
    "## Data description\n",
    "\n",
    "Clinical notes and metadata associated with 111,000 visits to the doctor were analyzed.\n",
    "\n",
    "## Analytic approach\n",
    "\n",
    "Natural Language Processing (NLP) was applied to the clinical notes to extract informative content. Specifically, the NLP tool spaCy was used to build a model that could identify and label specific spans of the text that held information of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c3d52-681a-4702-946a-58424d90ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import DocBin\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a617c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb2f37",
   "metadata": {},
   "source": [
    "## Import data \n",
    "\n",
    "#### Load variable mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"primary_data/variable_mapping.json\").open(\"r\") as f:\n",
    "    mapping = json.load(f, parse_int=True)\n",
    "\n",
    "# convert the encoded values in the mapping to integers since they get read in as strings\n",
    "for c in mapping.keys():\n",
    "    mapping[c] = {int(k): v for k, v in mapping[c].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dbb9d",
   "metadata": {},
   "source": [
    "#### Load primary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fddc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"primary_data/primary_data.csv\",\n",
    "    # set columns that can be null to nullable ints\n",
    "    dtype={\"body_part_2\": \"Int64\", \"diagnosis_2\": \"Int64\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e80fbe",
   "metadata": {},
   "source": [
    "#### Replace numeric values with corresponding strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df = df.copy()\n",
    "\n",
    "for col in mapping.keys():\n",
    "    decoded_df[col] = decoded_df[col].map(mapping[col])\n",
    "\n",
    "# Check on the df\n",
    "decoded_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79536b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure mappings were applied correctly by checking that the number of missing values did not change\n",
    "assert (decoded_df.isnull().sum() == df.isnull().sum()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21866a57",
   "metadata": {},
   "source": [
    "Because the narrative text will be a main focus of this analysis, it deserves some investigation. First, create a column that holds how many characters are in each narrative text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c7795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add character length column\n",
    "decoded_df['narrative_characters'] = decoded_df['narrative'].str.len()\n",
    "\n",
    "# Exploratory plot of distribution\n",
    "sns.kdeplot(decoded_df['narrative_characters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33cdb2",
   "metadata": {},
   "source": [
    "Find the value that occurs most frequently, called the mode, which is the point at the top of the curve. Also find the median of the character lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3bda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded_df['narrative_characters'].mode())\n",
    "print(decoded_df['narrative_characters'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318dc1a",
   "metadata": {},
   "source": [
    "By plotting this for the male and female sexes separately, we can see how the two compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(decoded_df, x = 'narrative_characters', hue = 'sex', kind = 'kde', fill = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file if one wants to come back at this point\n",
    "# decoded_df.to_csv(\"decoded_primary_data_with_char_lens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c1651",
   "metadata": {},
   "source": [
    "## Narrative manipulation\n",
    "Convert narrative text so that the most common abbreviations become words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_terms = {\n",
    "    \"&\": \"and\",\n",
    "    \"***\": \"\",\n",
    "    \">>\": \"clinical diagnosis\",\n",
    "    \"@\": \"at\",\n",
    "    \"+\": \"with\",\n",
    "    \"?\": \"unknown if\",\n",
    "    \"abd\": \"abdomen\",\n",
    "    \"af\": \"accidental fall\",\n",
    "    \"afib\": \"atrial fibrillation\",\n",
    "    \"alf\": \"assisted living facility\",\n",
    "    \"aki\": \"acute kidney injury\",\n",
    "    \"am\": \"morning\",\n",
    "    \"ams\": \"altered mental status\",\n",
    "    \"bac\": \"blood alcohol content\",\n",
    "    \"biba\": \"brought in by ambulance\",\n",
    "    \"bwd\": \"backwards\",\n",
    "    \"c/o\": \"complains of\",\n",
    "    \"chi\": \"closed-head injury\",\n",
    "    \"clsd\": \"closed\",\n",
    "    \"cpk\": \"creatine phosphokinase\",\n",
    "    \"cva\": \"cerebral vascular accident\",\n",
    "    \"dn\": \"down\",\n",
    "    \"dtr\": \"daughter\",\n",
    "    \"dx\": \"clinical diagnosis\",\n",
    "    \"ecf\": \"extended care facility\",\n",
    "    \"er\": \"emergency room\",\n",
    "    \"etoh\": \"ethyl alcohol\",\n",
    "    \"eval\": \"evaluation\",\n",
    "    \"fd\": \"found\",\n",
    "    \"ft\": \"foot\",\n",
    "    \"fx\": \"fracture\",\n",
    "    \"fxs\": \"fractures\",\n",
    "    \"fwd\": \"forwards\",\n",
    "    \"glf\": \"ground level fall\",\n",
    "    \"h/o\": \"history of\",\n",
    "    \"hr\": \"hours\",\n",
    "    \"htn\": \"hypertension\",\n",
    "    \"hx\": \"history of\",\n",
    "    \"inj\": \"injury\",\n",
    "    \"inr\": \"international normalized ratio\",\n",
    "    \"intox\": \"intoxication\",\n",
    "    \"lac\": \"laceration\",\n",
    "    \"loc\": \"loss of consciousness\",\n",
    "    \"lt\": \"left\",\n",
    "    \"mech\": \"mechanical\",\n",
    "    \"mult\": \"multiple\",\n",
    "    \"n h \": \"nursing home\",\n",
    "    \"nh\": \"nursing home\",\n",
    "    \"p/w\": \"presents with\",\n",
    "    \"pm\": \"afternoon\",\n",
    "    \"pt\": \"patient\",\n",
    "    \"pta\": \"prior to arrival\",\n",
    "    \"pts\": \"patient's\",\n",
    "    \"px\": \"physical examination\", # not \"procedure\",\n",
    "    \"r/o\": \"rules out\",\n",
    "    \"rt\": \"right\",\n",
    "    \"s/p\": \"after\",\n",
    "    \"sah\": \"subarachnoid hemorrhage\",\n",
    "    \"sdh\": \"acute subdural hematoma\",\n",
    "    \"sts\": \"sit to stand\",\n",
    "    \"tr\": \"trauma\",\n",
    "    \"uti\": \"urinary tract infection\",\n",
    "    \"unwit'd\": \"unwitnessed\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"w/\": \"with\",\n",
    "    \"wks\": \"weeks\"\n",
    "}\n",
    "\n",
    "def add_space_after(text, target=\",\"):\n",
    "    replaced_text = re.sub(r'({})(\\S)'.format(re.escape(target)), r'{} \\2'.format(target),text)\n",
    "    return replaced_text\n",
    "\n",
    "def add_space_before(text, target=\",\"):\n",
    "    replaced_text = re.sub(r'(\\S)({})'.format(re.escape(target)), r'\\1 {}'.format(target), text)\n",
    "    return replaced_text\n",
    "\n",
    "# cleanning\n",
    "def clean_narrative(text):\n",
    "    # lowercase everything\n",
    "    if pd.isna(text):\n",
    "        return \"NA\"\n",
    "    else:     \n",
    "        text = text.lower()\n",
    "        \n",
    "        # unglue DX\n",
    "        regex_dx = r\"([ˆ\\W]*(dx)[ˆ\\W]*)\"\n",
    "        text = re.sub(regex_dx, r\". dx: \", text)\n",
    "    \n",
    "        # remove age and sex identifications\n",
    "        ## regex to capture age and sex (not perfect but captures almost all of the cases)\n",
    "        regex_age_sex = r\"(\\d+)\\s*?(yof|yf|yo\\s*female|yo\\s*f|yom|ym|yo\\s*male|yo\\s*m)\"\n",
    "        age_sex_match = re.search(regex_age_sex, text)\n",
    "    \n",
    "        ## format age and sex\n",
    "        if age_sex_match:\n",
    "            age = age_sex_match.group(1)\n",
    "            sex = age_sex_match.group(2)\n",
    "            \n",
    "            # probably not best practice but it works with this data\n",
    "            if \"f\" in sex:\n",
    "                #text = text.replace(age_sex_match.group(0), f\"{age} years old female\")\n",
    "                text = text.replace(age_sex_match.group(0), f\"patient\")\n",
    "            elif \"m\" in sex:\n",
    "                #text = text.replace(age_sex_match.group(0), f\"{age} years old male\")\n",
    "                text = text.replace(age_sex_match.group(0), f\"patient\")\n",
    "                \n",
    "        text = add_space_after(text, target=\",\") \n",
    "        text = add_space_after(text, target=\";\") \n",
    "        text = add_space_after(text, target=\":\") \n",
    "        text = add_space_before(text, target=\"--\")  \n",
    "        text = add_space_after(text, target=\"--\")\n",
    "        text = re.sub(\"-\", \" \", text)\n",
    "        text = re.sub(r\"([0-9]+(\\.[0-9]+)?)\",r\" \\1 \", text).strip()\n",
    "        text = re.sub(\"\\.\", \" \", text)\n",
    "        \n",
    "        # The below two are separated from the dictionary because of the '&' character conflict\n",
    "        #   with the translate medical terms section use of '&'\n",
    "        text = re.sub(\"t'd&f\", \"tripped and fell\", text)\n",
    "        text = re.sub(\"s'd&f\", \"slipped and fell\",text)     \n",
    "    \n",
    "        \n",
    "        # translate medical terms\n",
    "        for term, replacement in medical_terms.items():\n",
    "            if term == \"@\" or term == \">>\" or term == \"&\" or term == \"***\" or term == \"+\" or term == \"?\":\n",
    "                pattern = fr\"({re.escape(term)})\"\n",
    "                text = re.sub(pattern, f\" {replacement} \", text) # force spaces around replacement\n",
    "                \n",
    "            else:\n",
    "                pattern = fr\"(?<!-)\\b({re.escape(term)})\\b(?!-)\"\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "                            \n",
    "        # This done after translate medical terms because some use a '/'\n",
    "        text = add_space_before(text, target=\"/\")  \n",
    "        text = add_space_after(text, target=\"/\")\n",
    "        \n",
    "        # remove extra white spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "        return text.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51f11b",
   "metadata": {},
   "source": [
    "Check how well that function works to change some complex text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '72 YOF SLIPPED&FELL OFF 4FT LADDER ONTO THE FLOOR THIS AM DTR FD DN+LOC>>L-3, L-4 FRACTURE, RT RIB FRACTURES X 3'\n",
    "print(\"Original text:\", text)\n",
    "print(\"Clean text:\", clean_narrative(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508e2f3",
   "metadata": {},
   "source": [
    "While it would probably be better for things like \"L3\" to remain together, I will sacrifice that for the easy advantage of separaing other number-character strings that would be better off separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5227f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data to just case id and narrative strings\n",
    "df_narratives = df.iloc[:, 0:2]\n",
    "\n",
    "# Apply the text cleaning function to all narrative entries\n",
    "df_narratives['narrative'] = df_narratives['narrative'].apply(lambda x: clean_narrative(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename original narrative column in the decoded df\n",
    "df_nn = decoded_df.rename(columns={\"narrative\": \"narrative_original\"})\n",
    "\n",
    "# Merge the modified narratives with the decoded dataframe\n",
    "df_final = pd.merge(df_narratives, df_nn, on=\"cpsc_case_number\", how=\"left\")\n",
    "\n",
    "# Check head to verify\n",
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file if one wants to come back at this point\n",
    "# df_final.to_csv(\"corrected_narrative_primary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e710b",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91e26a-632b-4f0a-bc62-a4ab88f338b5",
   "metadata": {},
   "source": [
    "Import semi-manually labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628b143-9f57-4b01-9e39-0846c0f2dd48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_sets = pd.read_csv('secondary_data/labeled_training_data.csv')\n",
    "labeled_sets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfc3bb",
   "metadata": {},
   "source": [
    "Check how long the imported data frame is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labeled_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c1560",
   "metadata": {},
   "source": [
    "Check how many unique cases are contained in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4489540-39a1-4ac2-90a9-b823642f9c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_list = labeled_sets.cpsc_case_number.unique()\n",
    "len(key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c859e5",
   "metadata": {},
   "source": [
    "Check how many cases in the training set have no labeled span. (Note, this is also an important component of one's training set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sets['span'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd8ca8",
   "metadata": {},
   "source": [
    "Randomly shuffle the key list in case there is some bias in the order of the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd976382",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(99)\n",
    "np.random.shuffle(key_list, )\n",
    "key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a8c26",
   "metadata": {},
   "source": [
    "The following chunk of code goes through the labeled training set, processing it to be a list of Spacy doc objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aca9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# keeping span token lengths to appropriately set config\n",
    "token_lengths = []\n",
    "\n",
    "# Keeping labels in case I want to check\n",
    "label_list = []\n",
    "\n",
    "# This will hold the processed string and span docs\n",
    "docs=[] \n",
    "\n",
    "for case in key_list:\n",
    "    \n",
    "    # Subest to just one case\n",
    "    temp_df = labeled_sets[labeled_sets.cpsc_case_number == case]\n",
    "    \n",
    "    # Get the narrative and process into a doc\n",
    "    nar = temp_df.iloc[0,1]\n",
    "    doc = nlp(nar)\n",
    "    \n",
    "    if len(temp_df)==1:    \n",
    "        if pd.isna(temp_df.iloc[0,2]):\n",
    "            print(\"this entry has no label\")\n",
    "            doc.spans[\"sc\"] = []\n",
    "            docs.append(doc)\n",
    "        else:     \n",
    "            span_text = temp_df.iloc[0,2]\n",
    "            temp_label = temp_df.iloc[0,3]   \n",
    "            span_start_char = nar.find(span_text)\n",
    "            span_end_char = span_start_char + len(span_text)\n",
    "            \n",
    "            # Finding the start and end tokens using character offsets\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx == span_start_char:\n",
    "                     start_token = token.i\n",
    "                if token.idx + len(token.text) == span_end_char:\n",
    "                    end_token = token.i\n",
    "                    break\n",
    "                    \n",
    "            if start_token is not None and end_token is not None:\n",
    "                temp_start = start_token\n",
    "                temp_end = end_token + 1\n",
    "                \n",
    "                doc.spans[\"sc\"] = [Span(doc, temp_start, temp_end, temp_label)]\n",
    "                \n",
    "                token_lengths.append(temp_end - temp_start)\n",
    "                label_list.append(temp_label)\n",
    "            \n",
    "                docs.append(doc)\n",
    "            else:\n",
    "                print(nar, \"span=\", span_text,\"couldn't find tokens\")\n",
    "    else:\n",
    "        print(\"temp_df has length > 1\")\n",
    "        span_list = []\n",
    "        for ent in range(len(temp_df)):\n",
    "            span_text = temp_df.iloc[ent,2]\n",
    "            temp_label = temp_df.iloc[ent,3]\n",
    "            span_start_char = nar.find(span_text)\n",
    "            span_end_char = span_start_char + len(span_text)\n",
    "            # print(span_start_char, span_end_char)\n",
    "\n",
    "            # Finding the start and end tokens using character offsets\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            for token in doc:\n",
    "                if token.idx == span_start_char:\n",
    "                     start_token = token.i\n",
    "                if token.idx + len(token.text) == span_end_char:\n",
    "                    end_token = token.i\n",
    "                    break\n",
    "            if start_token is not None and end_token is not None:\n",
    "                temp_start = start_token\n",
    "                temp_end = end_token + 1             \n",
    "                span_list.append(Span(doc, temp_start, temp_end, temp_label))\n",
    "                token_lengths.append(temp_end - temp_start)\n",
    "                label_list.append(temp_label)\n",
    "            else:\n",
    "                print(nar, \"span=\",span_text, \"couldn't find tokens\")\n",
    "        \n",
    "        doc.spans[\"sc\"] = span_list\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f187d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e39443",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs[0:1]:\n",
    "    print(doc, doc.spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef2bfe",
   "metadata": {},
   "source": [
    "Check on length of tokens in spans to set appropriate parameters in training config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(token_lengths), np.max(token_lengths), np.median(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a50f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.quantile(token_lengths, q =[0.05,0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997c54d",
   "metadata": {},
   "source": [
    "Want to split the data into train and test such that a single case is not split between the two. Check two consecutive rows to make sure they are separate cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc378242",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1101]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888d041",
   "metadata": {},
   "source": [
    "Make training and test sets with the docs, saving in spacy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa81e1-08f2-41ae-ae6f-d0d2787322b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin(docs=docs[0:1101])\n",
    "doc_bin.to_disk(\"./train_falling.spacy\")\n",
    "\n",
    "doc_bin = DocBin(docs=docs[1101:])\n",
    "doc_bin.to_disk(\"./dev_falling.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265e138",
   "metadata": {},
   "source": [
    "Initialize the training config file. This will get some manual adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f34717",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config configs/config_spancat_singlelabel.cfg --lang en --pipeline spancat_singlelabel --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301eeb3",
   "metadata": {},
   "source": [
    "These adjustments were made to the above config file:\n",
    "\n",
    "[components.spancat_singlelabel.suggester]   \n",
    "@misc = \"spacy.ngram_range_suggester.v1\"  \n",
    "min_size = 2 <---  \n",
    "max_size = 12 <---  \n",
    "\n",
    "[training.score_weights]  \n",
    "spans_sc_f = 0.3 <---  \n",
    "spans_sc_p = 0.5 <---  \n",
    "spans_sc_r = 0.2 <---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf60a35",
   "metadata": {},
   "source": [
    "Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a374663",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train configs/config_spancat_singlelabel.cfg --paths.train train_falling.spacy --paths.dev dev_falling.spacy --training.eval_frequency 100  --system.seed 99 --output spacy_falling_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e220f29-7abc-44bf-8a99-e4a017f84e01",
   "metadata": {},
   "source": [
    "## Using model \n",
    "\n",
    "Load the model and check its component steps,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f2df3-c0da-4f9a-ae84-c4dbea3bc93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp_spancat = spacy.load(\"spacy_falling_model/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spancat.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ef15d",
   "metadata": {},
   "source": [
    "**Test model on a subset of the samples**\n",
    "\n",
    "Take a random sample for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = df_final.sample(n=5000, replace=False, random_state=42)\n",
    "\n",
    "# Subset to just first two columns containing case # and narrative\n",
    "random_sample_sub = random_sample.iloc[:,0:2]\n",
    "\n",
    "# Check the result\n",
    "random_sample_sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['cpsc_case_number','text', 'span_text',  'span_label', ]\n",
    "output_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "for row in random_sample_sub.iloc[1000:1050:,0:2].iterrows():\n",
    "    cpsc = (row[1]['cpsc_case_number'])\n",
    "    text = (row[1]['narrative'])\n",
    "    doc = nlp_spancat(text)\n",
    "\n",
    "    \n",
    "    if len(doc.spans[\"sc\"]) == 0:\n",
    "        df2 = pd.DataFrame([[cpsc,text, \"NA\", \"NA\"]], columns= cols)\n",
    "        # Append the new row to the DataFrame\n",
    "        output_df = pd.concat([output_df, df2])\n",
    "    else:\n",
    "        for span in doc.spans[\"sc\"]:\n",
    "        # Create a new row as a dictionary\n",
    "        \n",
    "            df2 = pd.DataFrame([[cpsc,text, span.text, span.label_,]], columns=cols)\n",
    "            # Append the new row to the DataFrame\n",
    "            output_df = pd.concat([output_df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251309c",
   "metadata": {},
   "source": [
    "## Run model on all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cf499",
   "metadata": {},
   "source": [
    "Before running on all samples, double check everything is in working order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df.iloc[0:3:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f62368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in decoded_df.iloc[0:12:,0:2].iterrows():\n",
    "    cpsc = (row[1]['cpsc_case_number'])\n",
    "    text = (row[1]['narrative'])\n",
    "    doc = nlp_spancat(text)\n",
    "    print(doc.spans[\"sc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with column names\n",
    "cols = ['cpsc_case_number','text', 'span_text', 'span_label']\n",
    "output_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "for row in decoded_df.iloc[:,0:2].iterrows():\n",
    "    cpsc = (row[1]['cpsc_case_number'])\n",
    "    text = (row[1]['narrative'])\n",
    "    doc = nlp_spancat(text)\n",
    "\n",
    "    \n",
    "    if len(doc.spans[\"sc\"]) == 0:\n",
    "        df2 = pd.DataFrame([[cpsc, text, \"NA\", \"NA\"]], columns=cols)\n",
    "        # Append the new row to the DataFrame\n",
    "        output_df = pd.concat([output_df, df2])\n",
    "    else:\n",
    "        for span in doc.spans[\"sc\"]:\n",
    "        # Create a new row as a dictionary\n",
    "        \n",
    "            df2 = pd.DataFrame([[cpsc, text, span.text, span.label_]], columns=cols)\n",
    "            # Append the new row to the DataFrame\n",
    "            output_df = pd.concat([output_df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af137a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file for coming back and starting up at this point\n",
    "# output_df.to_csv(\"predictions_falling_full_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5852af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"predictions_falling_full_set.csv\")\n",
    "output_df = temp_df.iloc[:,1:5]\n",
    "output_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128ce0c",
   "metadata": {},
   "source": [
    "### Process predicted spans into dataframes  \n",
    "\n",
    "Pivot the DataFrame so that each span label is a column and the span texts are the entries in those columns. A single narrative could have two occurrences of the same span label associated with it. Therefore, when collapsing each case into a single row, make a tuple with the individual span text entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_aggregator(series):\n",
    "    return tuple(series)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "pivot_output_df = output_df.pivot_table(index=['cpsc_case_number', 'text'], columns='span_label', values='span_text', aggfunc=tuple_aggregator).reset_index()\n",
    "pivot_output_df.columns.name = None \n",
    "pivot_output_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbda2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original DataFrame\n",
    "binary_df = pivot_output_df.copy()\n",
    "\n",
    "# Update the DataFrame to have 0 for NaN and 1 for actual values\n",
    "binary_df.iloc[:, 2:] = binary_df.iloc[:, 2:].map(lambda x: 0 if x == 'NA' or pd.isna(x) else 1)\n",
    "binary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a837c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_df = pd.merge(binary_df, decoded_df, how=\"left\", on=\"cpsc_case_number\")\n",
    "combo_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816126e7",
   "metadata": {},
   "source": [
    "### Words within labeled spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_entries = output_df[output_df['span_label'] == 'SO']\n",
    "len(SO_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_entries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1590772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8066ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_entries = output_df[output_df['span_label'] == 'OBJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8386e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_remove_OBJ = ['TRIPPED', 'OVER', 'A', 'AN','ON', 'THE', 'HER', \n",
    "                   'OF', 'HIS', 'TRIPPING', 'FALL', 'FELL', 'SLIPPED', 'OWN', 'FEET']\n",
    "OBJ_words = []\n",
    "for i in range(len(OBJ_entries)):\n",
    "    temp_entry = OBJ_entries.iloc[i,2]\n",
    "    temp_text  = temp_entry.split(' ')\n",
    "    out_temp = list(set(temp_text)-set(list_remove_OBJ))\n",
    "    OBJ_words.append(out_temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_word_counts = collections.Counter(flatten_list(OBJ_words))\n",
    "my_dict = dict(OBJ_word_counts)\n",
    "\n",
    "rows = []\n",
    "for key, value in my_dict.items():\n",
    "    row = {'element': key, 'count': value}\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows).sort_values('count', ascending=False)\n",
    "\n",
    "df.set_index('element', inplace=True)\n",
    "OBJ_plot_subset = df.head(13).sort_values('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_plot_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = OBJ_plot_subset.plot.barh(width = 0.8)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Object causing fall')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "plt.savefig(\"Object_causing.pdf\", format=\"pdf\", pad_inches = 0.5, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_remove = ['A', 'HEAD', 'HIT', 'THE', 'ON', 'HITTING', 'STRIKING', \n",
    "               'HER', 'OF', 'AGAINST', 'STRUCK', 'ONTO', 'HIS', 'INTO',\n",
    "              'FACE','BACK', 'SIDE', 'CHEST', 'RIGHT', 'LEFT', 'EDGE', \n",
    "               'FOREHEAD', 'STAND','FRAME', 'AND', 'COFFEE', 'LANDED', 'ARM'\n",
    "              ]\n",
    "SO_words = []\n",
    "for i in range(len(SO_entries)):\n",
    "    temp_entry = SO_entries.iloc[i,2]\n",
    "    temp_text  = temp_entry.split(' ')\n",
    "    out_temp = list(set(temp_text)-set(list_remove))\n",
    "    SO_words.append(out_temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14190be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "              \n",
    "SO_word_counts = collections.Counter(flatten_list(SO_words))\n",
    "my_dict = dict(SO_word_counts)\n",
    "\n",
    "rows = []\n",
    "for key, value in my_dict.items():\n",
    "    row = {'element': key, 'count': value}\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows).sort_values('count', ascending=False)\n",
    "\n",
    "df.set_index('element', inplace=True)\n",
    "SO_plot_subset = df.head(15).sort_values('count')\n",
    "SO_plot_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = SO_plot_subset.plot.barh(width = 0.8)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Object struck')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "plt.savefig(\"Object_struck.pdf\", format=\"pdf\", pad_inches = 0.5, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "# fig.suptitle('Falls involving objects', size=20, y=1.02)\n",
    "OBJ_plot_subset.plot.barh(width = 0.8, ax = ax1, color = '#4893ff')\n",
    "SO_plot_subset.plot.barh(width = 0.8, ax = ax2, color = '#7cd357')\n",
    "ax1.legend().set_visible(False) \n",
    "ax2.legend().set_visible(False) \n",
    "ax1.set_ylabel('')\n",
    "ax2.set_ylabel('')\n",
    "ax1.tick_params(labelsize=12)\n",
    "ax2.tick_params(labelsize=12)\n",
    "ax1.set_title(\"Object instigating fall\", fontsize = 16, color = '#0001ac')\n",
    "ax2.set_title(\"Object struck\", fontsize = 16, color = '#216326')\n",
    "ax1.set_xlabel('Count', fontsize = 14, color = '#0001ac')\n",
    "ax2.set_xlabel('Count', fontsize = 14, color = '#216326')\n",
    "ax1.grid(True, which='both', axis='x', linestyle='--', linewidth=0.8, color='lightgray')\n",
    "ax2.grid(True, which='both', axis='x', linestyle='--', linewidth=0.8, color='lightgray')\n",
    "ax1.tick_params(axis='y', labelcolor='#0001ac')\n",
    "ax2.tick_params(axis='y', labelcolor='#216326')\n",
    "ax1.tick_params(axis='x', labelcolor='#0001ac')\n",
    "ax2.tick_params(axis='x', labelcolor='#216326')\n",
    "for spine in ['left', 'right', 'top', 'bottom']:\n",
    "    ax1.spines[spine].set_edgecolor('#0001ac')\n",
    "    ax2.spines[spine].set_edgecolor('#216326')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.35)  #\n",
    "plt.savefig(\"Objects_associated_with_falls.pdf\", format=\"pdf\", pad_inches = 0.25, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2912648",
   "metadata": {},
   "source": [
    "### Clean labels for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the starting numbers from the category codes\n",
    "combo_df['diagnosis'] = combo_df['diagnosis'].str.replace(r'\\d+ - ', '', regex=True)\n",
    "combo_df['disposition'] = combo_df['disposition'].str.replace(r'\\d+ - ', '', regex=True)\n",
    "\n",
    "# Change the diagnosis codes to sentence case\n",
    "combo_df['diagnosis']= combo_df['diagnosis'].str.title()\n",
    "print(combo_df['disposition'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten the disposition names, keeping general meaning\n",
    "label_mapping = {'TREATED AND ADMITTED/HOSPITALIZED': 'Admitted', \n",
    "                 'TREATED/EXAMINED AND RELEASED': 'Released', \n",
    "                 'HELD FOR OBSERVATION': 'Held', \n",
    "                 'TREATED AND TRANSFERRED': 'Transferred',  \n",
    "                 'LEFT WITHOUT BEING SEEN': 'Unseen', }\n",
    "combo_df['disposition'] = combo_df['disposition'].map(label_mapping)\n",
    "print(combo_df['disposition'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42905721",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(combo_df, x = 'narrative_characters', hue = 'disposition', kind = 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2fd21",
   "metadata": {},
   "source": [
    "Gather rows for specific strike types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_involved = combo_df[combo_df['OBJ'] == 1]\n",
    "len(object_involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "struck_object = combo_df[(combo_df['SO']==1) & (combo_df['SF']!=1) & (combo_df['SU']!=1)]\n",
    "struck_floor = combo_df[(combo_df['SO']!=1) & (combo_df['SF']==1) & (combo_df['SU']!=1)]\n",
    "struck_unknown = combo_df[(combo_df['SO']!=1) & (combo_df['SF']!=1) & (combo_df['SU']==1)]\n",
    "print(len(struck_floor), len(struck_object), len(struck_unknown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9682e",
   "metadata": {},
   "source": [
    "Bind the struck object and struck floor sets back together for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e22ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "struck_object = struck_object.assign(struck = 'Object')\n",
    "struck_floor = struck_floor.assign(struck = 'Floor')\n",
    "striking_df = pd.concat([struck_object, struck_floor ], ignore_index=True) \n",
    "striking_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e6a51",
   "metadata": {},
   "source": [
    "Plot disposition distribution for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1edf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_disp = striking_df[['disposition', 'struck' ]]\n",
    "strike_disp_counts = strike_disp.groupby(['struck', 'disposition']).size().reset_index(name='count')\n",
    "temp = strike_disp_counts.pivot(columns = 'disposition', index = 'struck', values = 'count' )\n",
    "temp.columns.name=''\n",
    "col_order = ['Released', 'Admitted', 'Held', 'Transferred', 'Unseen']\n",
    "strike_disp_counts = temp[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be36d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "#sns.set_palette('colorblind')\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# Convert to relative proportions \n",
    "strike_disp_counts.apply(lambda x: x*100/sum(x), axis = 1).plot(kind = 'bar', stacked = True, fontsize = 14, edgecolor='none')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2) \n",
    "plt.ylabel(\"Percent\", size = 16)\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"Disposition_by_strike.pdf\", format=\"pdf\", pad_inches = 0.5, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be909a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_cols_to_use = ['Internal Injury',\n",
    " 'Fracture',\n",
    " 'Laceration',\n",
    " 'Contusions, Abr.',\n",
    " 'Avulsion',\n",
    " 'Hematoma',\n",
    " 'Concussion',\n",
    " 'Strain, Sprain',\n",
    " 'Dislocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "striking_other = striking_df[striking_df['diagnosis'].isin(diag_cols_to_use) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906cc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_diag_counts = striking_other.groupby(['struck', 'diagnosis']).size().reset_index(name='count').groupby(['struck']).sum('count')\n",
    "other_diag_counts.reset_index()\n",
    "other_diag_counts.rename(columns = {'count': 'Other'}, inplace = True)\n",
    "other_diag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_diag = striking_df[['diagnosis', 'struck' ]]\n",
    "strike_diag_sub = strike_diag[strike_diag['diagnosis'].isin(diag_cols_to_use)]\n",
    "strike_diag_counts = strike_diag_sub.groupby(['struck', 'diagnosis']).size().reset_index(name='count')\n",
    "temp = strike_diag_counts.pivot(columns = 'diagnosis', index = 'struck', values = 'count' )\n",
    "temp.columns.name=''\n",
    "temp_ordered = temp[diag_cols_to_use]\n",
    "strike_diag_counts = temp_ordered.merge(other_diag_counts, left_on = 'struck', right_on = 'struck')\n",
    "strike_diag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eff026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.set_style(\"whitegrid\")\n",
    "#sns.set_palette('deep')\n",
    "plt.style.use('tableau-colorblind10')\n",
    "strike_diag_counts.apply(lambda x: x*100/sum(x), axis = 1).plot(kind = 'bar', stacked = True, fontsize = 14, edgecolor = 'none')\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2) \n",
    "plt.ylabel(\"Percent\", size = 16)\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"Diagnosis_by_strike.pdf\", format=\"pdf\", pad_inches = 0.5, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n",
    "strike_disp_counts.apply(lambda x: x*100/sum(x), axis = 1).plot(kind = 'bar', ax = ax1, stacked = True, fontsize = 14, edgecolor = 'none')\n",
    "strike_diag_counts.apply(lambda x: x*100/sum(x), axis = 1).plot(kind = 'bar', ax = ax2, stacked = True, fontsize = 14, edgecolor = 'none')\n",
    "ax1.legend().set_bbox_to_anchor((1, 1))\n",
    "ax2.legend().set_bbox_to_anchor((1, 1))\n",
    "plt.subplots_adjust(wspace=1.25)  #\n",
    "plt.savefig(\"Struck_object_Disp_Diag.pdf\", format=\"pdf\", pad_inches = 0.25, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88585ee6",
   "metadata": {},
   "source": [
    "Look at frequencies of dispositon and diagnosis by strike type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_disp = struck_floor['disposition'].value_counts()\n",
    "sf_disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6ec0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "so_disp = struck_object['disposition'].value_counts()\n",
    "print(so_disp[\"Released\"], so_disp[\"Admitted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe9162",
   "metadata": {},
   "source": [
    "Use an odds ratio contigency table to determine if the striking the floor has a higher risk of hospitalization than striking an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f38082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import odds_ratio\n",
    "res = odds_ratio([[sf_disp[\"Admitted\"],so_disp[\"Admitted\"]],[sf_disp[\"Released\"],so_disp[\"Released\"]]])\n",
    "print(res.statistic, res.confidence_interval(confidence_level=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddcbeb",
   "metadata": {},
   "source": [
    "The odds of being admitted/hospitalized if a patient strikes the floor are 1.xx times (95% CI = 1.xx-1.xx) that of being admitted if a patient strikes an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "struck_unknown['disposition'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_diag = struck_object['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_diag = struck_floor['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_diag.sum() - sf_diag[\"Fracture\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f622780",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = odds_ratio([[sf_diag[\"Fracture\"],so_diag[\"Fracture\"]],[sf_diag.sum() - sf_diag[\"Fracture\"], so_diag.sum() - so_diag[\"Fracture\"]]])\n",
    "print(res.statistic, res.confidence_interval(confidence_level=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = odds_ratio([[sf_diag[\"Laceration\"],so_diag[\"Laceration\"]],[sf_diag.sum() - sf_diag[\"Laceration\"], so_diag.sum() - so_diag[\"Laceration\"]]])\n",
    "print(res.statistic, res.confidence_interval(confidence_level=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb94bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "struck_unknown['diagnosis'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead52870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another possible version of a stacked bar plot\n",
    "# struck_floor.groupby('disposition')['diagnosis'].value_counts(normalize=True).unstack('diagnosis').plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "widgets.ColorPicker(concise=False, description='Pick a color',value='blue',disabled=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
